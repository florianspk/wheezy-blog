---
title: "Argo Workflows : orchestrer vos jobs Kubernetes comme un pro"
date: 2025-09-19
summary: "Introduction compl√®te √† Argo Workflows avec un exemple simple et reproductible"
tags: ["Kubernetes", "GitOps", "DevOps", "Argo"]
categories: ["Automation"]
featuredImage: "featured.png"
---

# Introduction

## üéØ Pourquoi Argo Workflows ?

Dans le monde Kubernetes, automatiser des processus complexes est un v√©ritable d√©fi.
Que ce soit pour :
- Des **pipelines CI/CD**
- Des **jobs de traitement de donn√©es**
- Du **Machine Learning**
- Ou la **migration d'applications legacy**

On a besoin d'une solution capable de **d√©finir, planifier et ex√©cuter des workflows complexes** directement dans Kubernetes.

C'est exactement ce que fait **Argo Workflows**.
C'est un **orchestrateur Kubernetes-native**, con√ßu pour d√©composer vos jobs en **√©tapes (steps)**, connect√©es entre elles, le tout d√©crit en YAML.

En d'autres termes :
- üìù D√©finition d√©clarative ‚Üí 100% GitOps-friendly
- ‚ö° Scalabilit√© native gr√¢ce √† Kubernetes
- üîó Int√©gration parfaite avec le reste de la suite Argo (Events, CD)

---

## üß© Architecture d'Argo Workflows

Argo Workflows s'appuie sur plusieurs CRDs Kubernetes :

| **Composant**       | **R√¥le** |
|---------------------|----------|
| **Workflow**        | La ressource principale : d√©crit le pipeline √† ex√©cuter |
| **WorkflowTemplate**| Template r√©utilisable de workflow |
| **CronWorkflow**    | Pour ex√©cuter un workflow selon un horaire |
| **WorkflowController** | Le contr√¥leur qui orchestre l'ex√©cution des workflows |

üí° **Concept cl√©** : chaque √©tape d'un workflow est ex√©cut√©e dans un **Pod Kubernetes**, ce qui permet une isolation forte et une scalabilit√© parfaite.


<div style="display: flex; gap: 20px; justify-content: space-around; align-items: flex-start; flex-wrap: wrap;">

<div style="flex: 1; min-width: 300px; text-align: center;">

### Workflow simple
{{< mermaid >}}
flowchart TD
    A[Start] --> B[Step 1 : Extract data]
    B --> C[Step 2 : Transform data]
    C --> D[Step 3 : Load to Database]
    D --> E[End]
{{< /mermaid >}}

</div>

<div style="flex: 1; min-width: 300px; text-align: center;">

### Workflow DAG
{{< mermaid >}}
flowchart TD
    A[Start] --> B[Step 1 : Fetch raw data]
    B --> C[Step 2A : Clean data]
    B --> D[Step 2B : Generate metadata]
    C --> E[Step 3 : Aggregate & validate]
    D --> E
    E --> F[Step 4 : Load to production DB]
    F --> G[End]
{{< /mermaid >}}

</div>

</div>






---

# ‚öôÔ∏è Installation

> **Pr√©-requis** : un cluster Kubernetes fonctionnel et `kubectl`.

1. **Cr√©er un namespace d√©di√©**
{{< highlight bash >}}
kubectl create namespace argo-workflows
{{< /highlight >}}


2. **Installer Argo Workflows**
{{< highlight bash >}}
kubectl apply -n argo-workflows -f https://raw.githubusercontent.com/argoproj/argo-workflows/stable/manifests/quick-start-minimal.yaml
{{< /highlight >}}


3. **V√©rifier que tout est OK**
{{< highlight bash >}}
kubectl get pods -n argo-workflows
{{< /highlight >}}


Vous devriez voir :
```
workflow-controller-xxxx    Running
argo-server-xxxx            Running
```

---

# üé¨ Exemple live : workflow basique

Objectif de la d√©mo :
- D√©finir un workflow qui ex√©cute **deux √©tapes successives** :
  1. Afficher "Hello"
  2. Afficher "World"

---

## Cr√©er le Workflow

{{< highlight yaml >}}
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: data-pipeline-dag-
spec:
  entrypoint: data-pipeline-dag
  templates:
  - name: data-pipeline-dag
    dag:
      tasks:
      - name: fetch-data
        template: fetch-data

      - name: clean-data
        dependencies: [fetch-data]
        template: clean-data

      - name: generate-metadata
        dependencies: [fetch-data]
        template: generate-metadata

      - name: aggregate-validate
        dependencies: [clean-data, generate-metadata]
        template: aggregate-validate

      - name: load-production
        dependencies: [aggregate-validate]
        template: load-production

  # --- Templates (containers ex√©cut√©s) ---
  - name: fetch-data
    container:
      image: alpine:3.20
      command: [sh, -c]
      args: ["echo 'üì• Fetching raw data...' && sleep 2"]

  - name: clean-data
    container:
      image: alpine:3.20
      command: [sh, -c]
      args: ["echo 'üßπ Cleaning data...' && sleep 3"]

  - name: generate-metadata
    container:
      image: alpine:3.20
      command: [sh, -c]
      args: ["echo 'üìù Generating metadata...' && sleep 2"]

  - name: aggregate-validate
    container:
      image: alpine:3.20
      command: [sh, -c]
      args: ["echo 'üîó Aggregating and validating data...' && sleep 4"]

  - name: load-production
    container:
      image: alpine:3.20
      command: [sh, -c]
      args: ["echo 'üöÄ Loading data to production DB...' && sleep 2"]
{{< /highlight >}}


Appliquer le workflow :
{{< highlight yaml >}}
kubectl create -f data-pipeline-dag.yaml
{{< /highlight >}}


Lister les workflows :
{{< highlight bash >}}
kubectl get wf -n argo-workflows
{{< /highlight >}}


üéâ Votre premier workflow Kubernetes est op√©rationnel !

---

# ‚è∞ D√©clencher un workflow √† intervalles r√©guliers

Argo Workflows propose aussi les **CronWorkflows**, pour planifier l'ex√©cution de workflows.

```yaml
# cron-hello-world.yaml
apiVersion: argoproj.io/v1alpha1
kind: CronWorkflow
metadata:
  name: cron-hello-world
  namespace: argo-workflows
spec:
  schedule: "*/5 * * * *" # toutes les 5 minutes
  workflowSpec:
    entrypoint: main
    templates:
    - name: main
      container:
        image: alpine:3.18
        command: [echo]
        args: ["Hello every 5 minutes!"]
```

Appliquer :
```bash
kubectl apply -f cron-hello-world.yaml
```

V√©rifier le d√©clenchement automatique :
```bash
kubectl get wf -n argo-workflows
```

---

# üì¶ Les Artefacts : partager des donn√©es entre √©tapes

Les **artefacts** sont l'un des concepts les plus puissants d'Argo Workflows. Ils permettent de **partager des fichiers et donn√©es** entre diff√©rentes √©tapes du workflow, cr√©ant ainsi de v√©ritables pipelines de donn√©es.

## üîß Comment √ßa fonctionne ?

Un artefact peut √™tre :
- **Produit** par une √©tape (output)
- **Consomm√©** par une autre √©tape (input)
- **Stock√©** dans diff√©rents backends (S3, GCS, Azure, etc.)

### Exemple : Pipeline avec artefacts

{{< highlight yaml >}}
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: artifacts-pipeline-
spec:
  entrypoint: artifact-pipeline
  templates:
  - name: artifact-pipeline
    dag:
      tasks:
      - name: generate-data
        template: data-generator

      - name: process-data
        dependencies: [generate-data]
        template: data-processor
        arguments:
          artifacts:
          - name: input-data
            from: "{{tasks.generate-data.outputs.artifacts.raw-data}}"

      - name: analyze-results
        dependencies: [process-data]
        template: data-analyzer
        arguments:
          artifacts:
          - name: processed-data
            from: "{{tasks.process-data.outputs.artifacts.clean-data}}"

  # --- G√©n√©rateur de donn√©es ---
  - name: data-generator
    container:
      image: alpine:3.20
      command: [sh, -c]
      args: |
        - echo "üîß Generating raw data..."
        - mkdir -p /tmp/output
        - echo "user1,25,engineer" > /tmp/output/data.csv
        - echo "user2,30,designer" >> /tmp/output/data.csv
        - echo "user3,28,manager" >> /tmp/output/data.csv
        - ls -la /tmp/output/
    outputs:
      artifacts:
      - name: raw-data
        path: /tmp/output
        archive:
          none: {}

  # --- Processeur de donn√©es ---
  - name: data-processor
    inputs:
      artifacts:
      - name: input-data
        path: /tmp/input
    container:
      image: alpine:3.20
      command: [sh, -c]
      args: |
        - echo "üßπ Processing input data..."
        - ls -la /tmp/input/
        - cat /tmp/input/data.csv
        - mkdir -p /tmp/output
        - echo "name,age,role,status" > /tmp/output/processed.csv
        - sed 's/$/,active/' /tmp/input/data.csv >> /tmp/output/processed.csv
        - echo "‚úÖ Data processed successfully"
    outputs:
      artifacts:
      - name: clean-data
        path: /tmp/output
        archive:
          none: {}

  # --- Analyseur de r√©sultats ---
  - name: data-analyzer
    inputs:
      artifacts:
      - name: processed-data
        path: /tmp/analysis
    container:
      image: alpine:3.20
      command: [sh, -c]
      args: |
        - echo "üìä Analyzing processed data..."
        - echo "Input files:"
        - ls -la /tmp/analysis/
        - echo "Content analysis:"
        - wc -l /tmp/analysis/processed.csv
        - echo "‚úÖ Analysis complete!"
{{< /highlight >}}

## üóÇÔ∏è Types de stockage d'artefacts

Argo Workflows supporte plusieurs backends pour stocker vos artefacts :

| **Backend** | **Description** | **Cas d'usage** |
|-------------|-----------------|-----------------|
| **S3** | Amazon S3 ou compatible | Production, donn√©es volumineuses |
| **GCS** | Google Cloud Storage | Environnements GCP |
| **Azure** | Azure Blob Storage | Environnements Azure |
| **Git** | D√©p√¥t Git | Configuration, scripts |
| **HTTP** | Serveur HTTP/HTTPS | APIs externes |

### Configuration S3 (exemple)

{{< highlight yaml >}}
# Configuration globale dans le ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: workflow-controller-configmap
  namespace: argo-workflows
data:
  config: |
    artifactRepository:
      s3:
        bucket: my-argo-artifacts
        endpoint: s3.amazonaws.com
        accessKeySecret:
          name: argo-artifacts
          key: accesskey
        secretKeySecret:
          name: argo-artifacts
          key: secretkey
{{< /highlight >}}

## üí° Bonnes pratiques

### 1. Optimiser la taille des artefacts
```yaml
outputs:
  artifacts:
  - name: logs
    path: /tmp/logs
    archive:
      tar:
        compressionLevel: 9  # Compression maximale
```

### 2. Artefacts conditionnels
```yaml
outputs:
  artifacts:
  - name: error-logs
    path: /tmp/errors
    optional: true  # N'√©choue pas si le fichier n'existe pas
```

### 3. Nettoyage automatique
```yaml
metadata:
  labels:
    workflows.argoproj.io/archive-strategy: "false"
spec:
  ttlStrategy:
    secondsAfterCompletion: 3600  # Supprime apr√®s 1h
```

---

# üåê Interface graphique

Argo Workflows poss√®de une **UI tr√®s intuitive** pour visualiser et suivre vos workflows.

1. **Acc√©der √† l'interface graphique :**
```bash
kubectl -n argo-workflows port-forward svc/argo-server 2746:2746
```

2. **Ouvrir dans votre navigateur :**
[http://localhost:2746](http://localhost:2746)

Vous pourrez y voir :
- L'historique des workflows
- Les logs de chaque √©tape
- La visualisation graphique des d√©pendances

---

# üîó Cas d‚Äôusages avanc√©s

Argo Workflows est extr√™mement flexible.
Quelques id√©es pour aller plus loin :

- **CI/CD GitOps**
  D√©ployer vos applications avec Argo Workflows en compl√©ment d'ArgoCD.

- **Machine Learning (MLOps)**
  Orchestrer des pipelines de training et de d√©ploiement de mod√®les.

- **Traitement de donn√©es**
  Lancer des jobs Spark ou ETL directement dans Kubernetes.

- **Automatisation d'infra**
  G√©n√©rer et appliquer des manifests Kubernetes via Terraform/Helm.

---

# ‚úÖ Conclusion

Argo Workflows est un outil puissant pour orchestrer vos pipelines dans Kubernetes :
- D√©finition **d√©clarative** et versionnable
- Ex√©cution **scalable** gr√¢ce aux Pods
- Int√©gration parfaite avec le reste de l'√©cosyst√®me Argo

Prochaine √©tape ?
- Lier Argo Workflows avec **Argo Events** pour du v√©ritable **event-driven orchestration**
