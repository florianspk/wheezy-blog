---
title: "Talos + Terraform = ‚ô•Ô∏è"
date: 2025-08-05
summary: "Mon retour d'exp√©rience sur Talos avec Terraform"
tags: ["Talos", "Kubernetes", "Terraform", "Infrastructure-as-Code", "Homelab"]
featuredImage: "featured.png"
---

{{< alert icon="üí°" title="√Ä noter" >}}
M√™me si je parle ici de **Terraform**, tout ce que je pr√©sente fonctionnerait √©galement avec **OpenTofu**, sans modification.
{{< /alert >}}

# Introduction

Depuis quelques mois, je m'int√©resse de pr√®s √† **Talos**, un syst√®me d'exploitation minimaliste con√ßu sp√©cifiquement pour Kubernetes.

J'ai install√© mon premier cluster Talos en **d√©cembre 2024**, et depuis, ma "production" ‚Äî compos√©e de deux serveurs physiques sous **Proxmox** ‚Äî tourne avec cette stack.

Apr√®s plusieurs mois d'exp√©rimentation, j'ai d√©cid√© de partager mon retour d'exp√©rience, notamment sur l'int√©gration de **Talos avec Terraform** pour provisionner et g√©rer mes clusters de mani√®re d√©clarative.

---

## ü§ñ Qu'est-ce que Talos ?

**Talos** est un OS Linux immuable, ultra-minimaliste, et totalement d√©di√© √† Kubernetes.
Oubliez les distributions classiques : pas de shell, pas de package manager, pas de surface d'attaque inutile.

Imaginez un OS qui d√©marre directement avec Kubernetes pr√©configur√© ! C'est exactement √ßa Talos. Pas besoin d'installer Docker, containerd, ou de configurer systemd ‚Äî tout est d√©j√† int√©gr√© et optimis√©.

Ce design √©pur√© apporte plusieurs avantages :

- ‚ö°Ô∏è **Installation en quelques minutes** : plus besoin de passer des heures √† configurer chaque n≈ìud
- üîê **Administration uniquement via une API d√©di√©e** : fini les connexions SSH hasardeuses
- üõ°Ô∏è **S√©curit√© renforc√©e** : surface d'attaque minimale, syst√®me en lecture seule

M√™me sans SSH, l'outil `talosctl` permet de tout g√©rer √† distance :

- üìú Acc√®s aux logs syst√®me (comme `journalctl` mais en mieux)
- üåê Collecte de rapports r√©seau (`pcap` pour d√©bugger les probl√®mes r√©seau)
- ü©∫ Diagnostic complet de l'√©tat des n≈ìuds
- üîÑ Mises √† jour du syst√®me en mode d√©claratif (comme Kubernetes !)

**SideroLabs**, l'entreprise derri√®re Talos, fournit une excellente documentation, et la communaut√© est active. C'est un projet mature, adopt√© en production par de nombreuses organisations ‚Äî pas juste un jouet pour homelab !

---

## üèóÔ∏è Pourquoi Terraform avec Talos ?

J'utilisais d√©j√† **Terraform** pour provisionner mes VMs sur Proxmox. Alors je me suis dit : *"Pourquoi ne pas aller plus loin et g√©rer aussi les configs Talos avec Terraform ?"*

L'id√©e peut para√Ætre folle au d√©but ‚Äî apr√®s tout, Talos a d√©j√† ses propres outils. Mais en r√©alit√©, cette approche r√©volutionne compl√®tement le workflow !

L'objectif : automatiser **tout** le cycle de vie du cluster, du boot au d√©ploiement des applications.

Ce que cette approche m'apporte concr√®tement :

- üîß **Versionner** toutes mes configurations dans Git (exit les fichiers YAML √©parpill√©s partout)
- üì¶ **Int√©grer mes charts Helm** d√®s l'initialisation (Cilium, ArgoCD, cert-manager... tout en une fois)
- üöÄ **Bootstrap automatique** du cluster (d√©marrage compl√®tement mains libres)
- üîÅ **D√©ploiement reproductible** √† 100% (m√™me apr√®s un wipe complet du homelab √† 2h du matin)

L'id√©e, c'est d'avoir un cluster Kubernetes **production-ready d√®s le premier boot** :
avec `Cilium` comme CNI, `ArgoCD` pour le GitOps, la gestion des certificats SSL et tous mes outils pr√©f√©r√©s d√©j√† en place. Plus besoin de se rappeler dans quel ordre installer quoi !

---

# üõ†Ô∏è Setup de Talos sans Terraform

Avant de plonger dans la magie Terraform, voyons comment Talos fonctionne "√† la main". C'est important de comprendre les bases !

Pour bootstrapper un cluster Talos classiquement, on utilise cette commande :

```bash
talosctl gen config talos-proxmox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out
```

**D√©cortiquons cette commande :**
- `talos-proxmox-cluster` : le nom de votre cluster (choisissez quelque chose de parlant !)
- `https://$CONTROL_PLANE_IP:6443` : l'URL o√π sera accessible l'API Kubernetes (remplacez par votre IP)
- `--output-dir _out` : le dossier o√π seront g√©n√©r√©s tous les fichiers

Dans le dossier `_out/`, on retrouve trois fichiers essentiels :

- `controlplane.yaml` ‚Äî La recette pour cuisiner vos n≈ìuds control plane
- `worker.yaml` ‚Äî La configuration des n≈ìuds worker (les petites mains du cluster)
- `talosconfig` ‚Äî Votre passe-partout pour parler avec `talosctl`

Ces fichiers contiennent **tous les secrets** de votre cluster : certificats, cl√©s, tokens... √Ä garder pr√©cieusement !

---

## ‚úçÔ∏è Champs importants de la configuration

Rentrons dans le vif du sujet ! Voici les sections cruciales de la config Talos :

### 1. Configuration du cluster

```yaml
cluster:
  clusterName: talos-proxmox-cluster
  controlPlane:
    endpoint: https://$CONTROL_PLANE_IP:6443
  network:
    dnsDomain: cluster.local
    podSubnets:
      - 10.244.0.0/16
    serviceSubnets:
      - 10.96.0.0/12
```

**Explications :**
- `clusterName` : le petit nom de votre cluster (sera visible dans `kubectl config`)
- `endpoint` : l'adresse pour acc√©der √† l'API Kubernetes depuis l'ext√©rieur
- `dnsDomain` : le suffixe DNS interne (g√©n√©ralement `cluster.local`)
- `podSubnets` : le r√©seau o√π vivront vos pods (pensez √† ne pas avoir de conflit avec votre LAN !)
- `serviceSubnets` : le r√©seau pour les services Kubernetes

### 2. Configuration machine

```yaml
machine:
  type: controlplane # ou worker
  token: wNf8GvZz... # Token d'authentification machine
  ca:
    crt: LS0tLS1CRU... # Certificat CA root du cluster
    key: LS0tLS1CRU... # Cl√© priv√©e CA (sensible!)
  certSANs:
    - $CONTROL_PLANE_IP
  kubelet:
    image: ghcr.io/siderolabs/kubelet:v1.x.x
```

**Ce qui se passe ici :**
- `type` : d√©finit le r√¥le de la machine (controlplane = chef d'orchestre, worker = ex√©cutant)
- `token` : un secret partag√© pour que les machines puissent se reconna√Ætre
- `ca.crt` et `ca.key` : les certificats racine du cluster (√† prot√©ger comme la prunelle de vos yeux !)
- `certSANs` : les adresses IP/DNS autoris√©es pour l'API (important pour √©viter les erreurs de certificats)
- `kubelet.image` : la version du kubelet √† utiliser

---

## üß© inlineManifests : La magie du d√©ploiement automatique

Les `inlineManifests` sont **LA feature killer** de Talos ! Ils permettent d'appliquer des manifests Kubernetes **d√®s le d√©marrage du cluster**.

Concr√®tement ? Votre cluster d√©marre avec Cilium, ArgoCD et tous vos outils d√©j√† install√©s. Plus besoin de faire 15 `kubectl apply` apr√®s chaque bootstrap !

Voici un exemple basique avec Flannel (CNI r√©seau) :

```yaml
cluster:
  inlineManifests:
    - name: "flannel"
      contents: |
        ---
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: kube-flannel-ds
          namespace: kube-system
        spec:
          selector:
            matchLabels:
              app: flannel
          # ... reste de la config Flannel
```

**Comment √ßa marche ?**
1. Talos d√©marre et initialise Kubernetes
2. D√®s que l'API est disponible, il applique automatiquement tous les `inlineManifests`
3. Votre cluster est imm√©diatement op√©rationnel avec votre stack pr√©f√©r√©e !

L'avantage √©norme : **reproductibilit√© totale**. Chaque fois que vous bootstrappez le cluster, vous obtenez exactement la m√™me chose.

---

# üì¶ Le truc magique : les Inline Manifests (avec Terraform cette fois !)

Maintenant qu'on a vu les bases, passons au niveau sup√©rieur ! L'id√©e g√©niale est de **g√©n√©rer dynamiquement** ces manifests via Terraform et Helm.

Au lieu d'√©crire √† la main des centaines de lignes YAML (bonjour l'enfer de la maintenance), on va laisser Helm faire le boulot et Terraform orchestrer le tout.

---

## üåê Providers Terraform utilis√©s

Pour cette aventure, j'ai besoin de deux providers Terraform :

```hcl
terraform {
  required_version = ">= 1.10.0"
  required_providers {
    talos = {
      source  = "siderolabs/talos"
      version = "0.8.1"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "3.0.2"
    }
  }
}
```

**Pourquoi ces deux-l√† ?**

- **Provider `talos`** : cr√©√© par SideroLabs eux-m√™mes, il permet de g√©n√©rer les configs machines Talos directement dans Terraform
- **Provider `helm`** : pour templater les charts Helm comme Cilium ou ArgoCD **sans d√©ployer** (on veut juste r√©cup√©rer le YAML final)

Cette combinaison est magique : Helm g√©n√®re le YAML parfait pour chaque chart, et le provider Talos l'injecte dans les `inlineManifests` !

---

## üîå Provisionnement de Cilium avec Helm + Terraform

**Cilium** est mon CNI de choix (Container Network Interface). C'est lui qui g√®re le r√©seau entre les pods et les services Kubernetes. Voici comment je l'int√®gre dans mes configs Talos :

```hcl
locals {
  # Quelques manifests personnalis√©s que Helm ne peut pas g√©n√©rer
  cilium_manifest_objects = [
    # Load Balancer pour exposer des services sur mon LAN
    {
      apiVersion = "cilium.io/v2alpha1"
      kind       = "CiliumL2AnnouncementPolicy"
      metadata = {
        name = "external"
      }
      spec = {
        loadBalancerIPs = true
        interfaces = ["eth0"]
        # Seuls les workers peuvent annoncer les IPs (pas les control planes)
        nodeSelector = {
          matchExpressions = [
            {
              key      = "node-role.kubernetes.io/control-plane"
              operator = "DoesNotExist"
            }
          ]
        }
      }
    },
    # Pool d'IPs disponibles pour le load balancer
    {
      apiVersion = "cilium.io/v2alpha1"
      kind       = "CiliumLoadBalancerIPPool"
      metadata = {
        name = "external"
      }
      spec = {
        blocks = [
          {
            start = "10.10.1.10"  # Premi√®re IP disponible
            stop  = "10.10.1.15"  # Derni√®re IP disponible
          }
        ]
      }
    }
  ]
  # Conversion en YAML pour les inlineManifests
  cilium_external_lb_manifest = join("---\n", [for d in local.cilium_manifest_objects : yamlencode(d)])
}

# La magie : Helm g√©n√®re le manifest Cilium complet
data "helm_template" "cilium" {
  namespace    = "kube-system"
  name         = "cilium"
  repository   = "https://helm.cilium.io"
  chart        = "cilium"
  version      = "1.18.0"
  kube_version = var.kubernetes_version
  values       = [file("${path.module}/helm/cilium-values.yaml")]
}
```

**Ce qui se passe ici :**

1. **Les `locals`** : je d√©finis des ressources Cilium sp√©cifiques que la chart Helm standard ne g√®re pas (comme le load balancer L2)
2. **Le `data "helm_template"`** : Terraform demande √† Helm de g√©n√©rer tout le YAML de Cilium, mais **sans le d√©ployer**
3. **Les `values`** : j'utilise un fichier `cilium-values.yaml` pour personnaliser Cilium (activation de Hubble, configuration r√©seau, etc.)

Le r√©sultat ? Un YAML parfait, test√© par la communaut√© Cilium, mais personnalis√© pour mon environnement !

---

# üß± Bootstrap des machine configs avec Terraform

Maintenant, le plat de r√©sistance ! Voici comment je g√©n√®re les configurations Talos avec Terraform :

```hcl
resource "talos_machine_secrets" "talos" {
  talos_version = "v${var.talos_version}"
}
```

Cette ressource g√©n√®re **tous les secrets** n√©cessaires au cluster : certificats CA, tokens d'authentification, cl√©s de chiffrement... C'est l'√©quivalent de ce que fait `talosctl gen config` mais dans Terraform.

Ensuite, je d√©finis la configuration des control planes :

```hcl
data "talos_machine_configuration" "controller" {
  cluster_name     = var.cluster_name
  cluster_endpoint = "https://${var.cluster_vip}:6443"
  machine_type     = "controlplane"
  machine_secrets  = talos_machine_secrets.talos.machine_secrets

  # Configuration r√©seau de base commune √† toutes les machines
  config_patches = [
    yamlencode(local.common_machine_config),
    # Configuration sp√©cifique aux control planes : VIP pour haute dispo
    yamlencode({
      machine = {
        network = {
          interfaces = [{
            interface = "eth0"
            vip = {
              ip = var.cluster_vip  # IP virtuelle partag√©e entre les control planes
            }
          }]
        }
      }
    }),
    # LE GROS MORCEAU : les inlineManifests avec Cilium
    yamlencode({
      cluster = {
        inlineManifests = [
          {
            name     = "cilium"
            contents = join("---\n", [
              data.helm_template.cilium.manifest,  # Le YAML g√©n√©r√© par Helm
              "# Configuration load balancer personnalis√©e\n${local.cilium_external_lb_manifest}",
            ])
          }
        ]
      }
    })
  ]
}
```

**D√©cryptage :**
- `cluster_endpoint` : l'adresse VIP (Virtual IP) partag√©e entre tous les control planes
- `config_patches` : des modifications YAML appliqu√©es par-dessus la config de base
- La **VIP** permet d'avoir plusieurs control planes derri√®re une m√™me IP (haute disponibilit√©)
- Les **`inlineManifests`** injectent directement le YAML de Cilium dans la config Talos

Et maintenant, l'application de ces configurations sur les vraies machines :

```hcl
# Application de la config sur chaque control plane
resource "talos_machine_configuration_apply" "controller" {
  count                       = var.controller_count
  client_configuration        = talos_machine_secrets.talos.client_configuration
  machine_configuration_input = data.talos_machine_configuration.controller.machine_configuration
  endpoint                    = local.controller_nodes[count.index].address
  node                        = local.controller_nodes[count.index].address

  # Patch sp√©cifique √† chaque machine (hostname, DNS, NTP)
  config_patches = [
    yamlencode({
      machine = {
        network = {
          hostname    = local.controller_nodes[count.index].name  # talos-cp-01, talos-cp-02, etc.
          nameservers = var.dns_serveurs                          # Mes DNS locaux
        }
        time = {
          servers = var.ntp_serveurs                              # Serveurs NTP pour la sync
        }
      }
    }),
  ]
}

# M√™me chose pour les workers (mais sans la VIP et les inlineManifests)
resource "talos_machine_configuration_apply" "worker" {
  count                       = var.worker_count
  client_configuration        = talos_machine_secrets.talos.client_configuration
  machine_configuration_input = data.talos_machine_configuration.worker.machine_configuration
  endpoint                    = local.worker_nodes[count.index].address
  node                        = local.worker_nodes[count.index].address

  config_patches = [
    yamlencode({
      machine = {
        network = {
          hostname    = local.worker_nodes[count.index].name
          nameservers = var.dns_serveurs
        }
        time = {
          servers = var.ntp_serveurs
        }
      }
    }),
  ]
}
```

**Ce qui se passe concr√®tement :**
1. Terraform g√©n√®re la config Talos avec Cilium int√©gr√©
2. Pour chaque machine, il applique cette config + des patches sp√©cifiques (nom, DNS, NTP)
3. Chaque n≈ìud red√©marre avec sa nouvelle configuration
4. Au boot, Talos applique automatiquement Cilium via les `inlineManifests`
5. Le cluster est op√©rationnel avec le r√©seau configur√© !

---

# üîß Upgrade du cluster via Terraform

## üÜô Comment je g√®re les upgrades

Fini les upgrades qui donnent des sueurs froides ! Avec Terraform + Talos, c'est d'une simplicit√© d√©concertante.

Dans mon fichier de variables, j'ai juste √ßa :

```hcl
variable "kubernetes_version" {
  description = "Version de Kubernetes √† d√©ployer"
  type        = string
  default     = "v1.33.3"
}
```

**Le workflow d'upgrade :**
1. Je change la version dans ma variable : `v1.33.2` ‚Üí `v1.33.3`
2. Je lance `terraform plan` pour voir ce qui va changer
3. Je fais `terraform apply`
4. Talos d√©tecte automatiquement la diff√©rence de version
5. Il upgrade n≈ìud par n≈ìud, en rolling update

C'est tout ! Pas de script bash chelou, pas de commandes √† retenir, pas de risque d'oublier un n≈ìud.

## ‚úÖ Mon retour d'exp√©rience apr√®s de nombreuses upgrades

**Ce que j'adore dans cette approche :**

- **Mise √† jour progressive et automatis√©e** : Talos upgrade un n≈ìud √† la fois, attend qu'il soit stable, puis passe au suivant
- **Zero downtime** : avec plusieurs control planes et workers, mes applications continuent de tourner
- **Rollback facile** : si √ßa part en vrille, je repasse √† l'ancienne version et `terraform apply`
- **Terraform garde l'√©tat √† jour** : plus jamais de d√©synchronisation entre mes fichiers et la r√©alit√©

**Un exemple concret :**
J'ai r√©cemment upgrad√© de Kubernetes 1.32 √† 1.33. Le processus a pris environ 20 minutes pour un cluster de 5 n≈ìuds, et je n'ai eu aucune interruption de service sur mes applications.

### ‚ö†Ô∏è Points de vigilance (j'ai appris √† mes d√©pens)

- **Toujours lire les release notes** : parfois il y a des breaking changes (surtout entre versions majeures)
- **Tester sur un cluster de dev d'abord** : Ca peut se faire tr√©s simplement en local
- **Sauvegarder les configs avant** : `git commit` de toutes vos configurations Terraform
- **Observer les logs pendant l'upgrade** : `talosctl logs -f` sur chaque n≈ìud pour voir si tout se passe bien
- **Pr√©voir du temps** : m√™me si c'est automatis√©, restez dispo pour surveiller

---

# üìÅ Mon Homelab en pratique

Si vous voulez voir tout √ßa en action, j'ai mis **tout mon code** en open source ! Vous y trouverez :

- Les configurations Terraform compl√®tes
- Les fichiers de values Helm pour chaque chart
- Mes scripts d'automatisation
- La documentation pour reproduire chez vous

{{< github repo="florianspk/home-lab-talos" >}}

**Ce que vous y trouverez concr√®tement :**
- Configuration Proxmox + Terraform pour cr√©er les VMs
- Bootstrap complet Talos avec Cilium, ArgoCD, cert-manager
- Ingress avec Traefik et certificats SSL auto
- Exemples d'applications d√©ploy√©es via GitOps

N'h√©sitez pas √† y jeter un ≈ìil et une star ‚≠êÔ∏è fait toujours plaisir (et aide d'autres personnes √† d√©couvrir le projet) !

---

# üéâ Bilan apr√®s 1 an avec Talos + Terraform

## ‚úÖ Les points positifs (et il y en a beaucoup !)

- **Simplicit√© d√©concertante** : plus d'OS √† patcher, configurer, maintenir
- **S√©curit√© renforc√©e** : syst√®me immuable, surface d'attaque minimale, pas de shell
- **Administration moderne** : tout passe par l'API avec `talosctl` (finies les connexions SSH hasardeuses)
- **Upgrades sans stress** : rolling updates automatiques, rollback facile
- **Int√©gration Terraform + Helm au top** : Infrastructure as Code pouss√©e √† son maximum
- **Reproductibilit√© parfaite** : je peux reconstruire mon cluster identique en 30 minutes
- **Communaut√© active** : documentation excellente, support r√©actif

## ü§î Quelques b√©mols (soyons honn√™tes)

- **Courbe d'apprentissage** : passer du SSH traditionnel √† l'API-only demande un changement d'habitudes
- **Debug parfois moins √©vident** : quand quelque chose ne va pas, il faut apprendre les outils sp√©cifiques √† Talos
- **√âcosyst√®me encore jeune** : moins de tutos et d'exemples que pour du Kubernetes avec kubeadm ou autre

## üöÄ Mes conseils pour bien commencer

1. **Testez d'abord sur un petit cluster** : 1 control plane + 1 worker dans des VMs
2. **Ma√Ætrisez `talosctl`** : prenez le temps d'explorer toutes les commandes
3. **G√©rez vos configs avec Terraform d√®s le d√©but** : n'attendez pas d'avoir 10 clusters en manuel
4. **Int√©grez vos charts Helm dans les inlineManifests** : c'est l√† que la magie op√®re

## üîÆ Et maintenant ?

Cette stack **Talos + Terraform + Helm** m'a vraiment r√©volutionn√© la gestion de mon/mes clusters. Mais je ne compte pas m'arr√™ter l√† !

**Prochaine √©tape** : creuser **Omni**, l'outil SaaS de gestion de clusters Talos par SideroLabs. L'id√©e : une interface web pour piloter tous mes clusters Talos, avec gestion des upgrades, monitoring int√©gr√© et d√©ploiement multi-cloud.

Si vous vous lancez dans l'aventure Talos, n'h√©sitez pas √† me faire un retour ! Je suis toujours curieux de voir comment d'autres personnes utilisent cette stack.
