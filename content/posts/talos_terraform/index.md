---
title: "Talos + Terraform = ‚ô•Ô∏è"
date: 2025-08-05
summary: "Mon retour d'exp√©rience sur Talos avec Terraform"
tags: ["Talos", "Kubernetes", "Terraform", "Infrastructure-as-Code", "Homelab"]
featuredImage: "featured.png"
---

{{< alert icon="üí°" title="√Ä noter" >}}
M√™me si je parle ici de **Terraform**, tout ce que je pr√©sente fonctionnerait √©galement avec **OpenTofu**, sans modification.
{{< /alert >}}

# Introduction

Depuis quelques mois, je m'int√©resse de pr√®s √† **Talos**, un syst√®me d'exploitation minimaliste con√ßu sp√©cifiquement pour Kubernetes.

J‚Äôai install√© mon premier cluster Talos en **d√©cembre 2024**, et depuis, ma "production" ‚Äî compos√©e de deux serveurs physiques sous **Proxmox** ‚Äî tourne avec cette stack.

Apr√®s plusieurs mois d‚Äôexp√©rimentation, j‚Äôai d√©cid√© de partager mon retour d‚Äôexp√©rience, notamment sur l'int√©gration de **Talos avec Terraform** pour provisionner et g√©rer mes clusters de mani√®re d√©clarative.

---

## ü§ñ Qu'est-ce que Talos ?

**Talos** est un OS Linux immuable, ultra-minimaliste, et totalement d√©di√© √† Kubernetes.
Oubliez les distributions classiques : pas de shell, pas de package manager, pas de surface d‚Äôattaque inutile.

Ce design √©pur√© apporte plusieurs avantages :

- ‚ö°Ô∏è Installation rapide et automatisable d‚Äôun n≈ìud Kubernetes
- üîê Administration uniquement via une **API d√©di√©e**, avec l‚Äôoutil `talosctl`

M√™me sans SSH, `talosctl` permet de tout g√©rer :

- üìú Acc√®s aux logs syst√®me
- üåê Collecte de rapports r√©seau (`pcap`)
- ü©∫ Diagnostic de l‚Äô√©tat des n≈ìuds
- üîÑ Mises √† jour du syst√®me en mode d√©claratif

**SideroLabs**, l‚Äôentreprise derri√®re Talos, fournit une excellente documentation, et la communaut√© est active. C‚Äôest un projet mature, adopt√© en production par de nombreuses organisations.

---

## üèóÔ∏è Pourquoi Terraform avec Talos ?

J‚Äôutilise **Terraform** pour provisionner mes VM sur Proxmox. Alors je me suis dit : *‚ÄúPourquoi ne pas aller plus loin et g√©rer aussi les configs Talos avec Terraform ?‚Äù*

L‚Äôobjectif : automatiser **tout** le cycle de vie du cluster, du boot au d√©ploiement des applications.

Ce que cette approche m‚Äôapporte :

- üîß **Versionner** toutes mes configurations dans Git
- üì¶ **Int√©grer mes charts Helm** d√®s l'initialisation
- üöÄ **Bootstrap automatique** du cluster
- üîÅ **D√©ploiement reproductible**, m√™me apr√®s un wipe complet

L‚Äôid√©e, c‚Äôest d‚Äôavoir un cluster Kubernetes **production-ready d√®s le premier boot** :
avec `Cilium`, `ArgoCD`, la gestion des certificats et mes outils pr√©f√©r√©s d√©j√† en place.

---


# üõ†Ô∏è Setup de Talos sans terraform

On vas rentrer dans le vif du sujet maintenant, comment j'ai vu les choses et comment un bootstrappe d'un cluster talos sans terraform
La commande `talosctl gen config talos-proxmox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out` g√©n√®re une configuration compl√®te pour un cluster Talos.
Voici les fichiers que nous allons retrouver dans `_out/` :
- `controlplane.yaml` - Configuration pour les n≈ìuds control plane
- `worker.yaml` - Configuration pour les n≈ìuds worker
- `talosconfig` - Fichier de configuration client pour talosctl

des certificats et mes outils pr√©f√©r√©s directement int√©gr√©s.

---

# üõ†Ô∏è Setup de Talos sans Terraform

On rentre maintenant dans le vif du sujet : comment booter un cluster Talos manuellement, sans Terraform.

La commande suivante g√©n√®re les fichiers de configuration pour un cluster Talos :

```bash
talosctl gen config talos-proxmox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out
```

Dans le dossier `_out/`, on retrouve :

- `controlplane.yaml` ‚Äî Configuration des n≈ìuds control plane
- `worker.yaml` ‚Äî Configuration des n≈ìuds worker
- `talosconfig` ‚Äî Fichier client pour `talosctl`

---

## ‚úçÔ∏è Champs importants de la configuration

### 1. Configuration du cluster

```yaml
cluster:
  clusterName: talos-proxmox-cluster
  controlPlane:
    endpoint: https://$CONTROL_PLANE_IP:6443
  network:
    dnsDomain: cluster.local
    podSubnets:
      - 10.244.0.0/16
    serviceSubnets:
      - 10.96.0.0/12
```

### 2. Configuration machine

```yaml
machine:
  type: controlplane # ou worker
  token: wNf8GvZz... # Token d'authentification machine
  ca:
    crt: LS0tLS1CRU... # Certificat CA root du cluster
    key: LS0tLS1CRU... # Cl√© priv√©e CA (sensible!)
  certSANs:
    - $CONTROL_PLANE_IP
  kubelet:
    image: ghcr.io/siderolabs/kubelet:v1.x.x
```

---

## üß© inlineManifests : D√©ploiement d√®s le bootstrap

Les `inlineManifests` permettent d‚Äôappliquer des manifests Kubernetes **d√®s le d√©marrage du cluster**. Parfait pour automatiser le bootstrap.

Exemple :

```yaml
cluster:
  inlineManifests:
    - name: "flannel"
      contents: |
        ---
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: kube-flannel-ds
          namespace: kube-system
        spec:
          ...
```

---

# üì¶ Le truc magique : les Inline Manifests (avec Terraform cette fois !)

Comme vu plus haut, les `inlineManifests` sont une feature g√©niale. L‚Äôid√©e maintenant est de **g√©n√©rer dynamiquement** ces manifests via Terraform et Helm.

---

## üåê Providers Terraform utilis√©s

Dans ce projet, j‚Äôutilise deux providers :

- `talos` ‚Äî pour cr√©er les configs machines Talos
- `helm` ‚Äî pour templater les charts Helm comme Cilium ou ArgoCD

```hcl
terraform {
  required_version = ">= 1.10.0"
  required_providers {
    talos = {
      source  = "siderolabs/talos"
      version = "0.8.1"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "3.0.2"
    }
  }
}
```

---

## üîå Provisionnement de Cilium avec Helm + Terraform

J‚Äôutilise **Cilium** comme CNI. Voici comment je d√©finis les manifests dans `cilium.tf` :

```hcl
locals {
  cilium_manifest_objects = [
    # Cr√©ation d'un objet CiliumL2AnnouncementPolicy non possible dans la chart helm
    {
      apiVersion = "cilium.io/v2alpha1"
      kind       = "CiliumL2AnnouncementPolicy"
      metadata = {
        name = "external"
      }
      spec = {
        loadBalancerIPs = true
        interfaces = ["eth0"]
        nodeSelector = {
          matchExpressions = [
            {
              key      = "node-role.kubernetes.io/control-plane"
              operator = "DoesNotExist"
            }
          ]
        }
      }
    },
    {
      apiVersion = "cilium.io/v2alpha1"
      kind       = "CiliumLoadBalancerIPPool"
      metadata = {
        name = "external"
      }
      spec = {
        blocks = [
          {
            start = cidrhost("10.10.1.1", 10)
            stop  = cidrhost("10.10.1.1", 15)
          }
        ]
      }
    }
  ]
  cilium_external_lb_manifest = join("---\n", [for d in local.cilium_manifest_objects : yamlencode(d)])
}
# C'est ici que se passe mon helm template
data "helm_template" "cilium" {
  namespace    = "kube-system"
  name         = "cilium"
  repository   = "https://helm.cilium.io"
  chart        = "cilium"
  version      = "1.18.0"
  kube_version = var.kubernetes_version
  values       = [file("${path.module}/helm/cilium-values.yaml")]
}
```

---

# üß± Bootstrap des machine configs avec Terraform

Dans un fichier `talos.tf`, je d√©finis la configuration des machines control planes et workers :

```hcl
resource "talos_machine_secrets" "talos" {
  talos_version = "v${var.talos_version}"
}

data "talos_machine_configuration" "controller" {
  ...
  # On peut meme lui passer des configs patches
  config_patches = [
    yamlencode(local.common_machine_config),
    yamlencode({
      machine = {
        network = {
          interfaces = [{
            interface = "eth0"
            vip = {
              ip = var.cluster_vip
            }
          }]
        }
      }
    }),
    yamlencode({
      cluster = {
        # Ici le fameux Block inlineManifests qui va me permettre d'appeler mes manifests g√©n√©r√©s juste avant
        inlineManifests = concat([
          {
            name     = "cilium"
            contents = join("---\n", [
              data.helm_template.cilium.manifest,
              "# Source cilium.tf\n${local.cilium_external_lb_manifest}",
            ])
          }
        ])
      }
    })
  ]
}
```

Et les ressources pour appliquer la configuration aux machines :

```hcl

resource "talos_machine_configuration_apply" "controller" {
  count                       = var.controller_count #Nombre de contr√¥leurs
  client_configuration        = talos_machine_secrets.talos.client_configuration #La config talos g√©n√©r√© au d√©but
  machine_configuration_input = data.talos_machine_configuration.controller.machine_configuration # le fichier machine configuration que nous avons g√©n√©r√© juste au dessus
  endpoint                    = local.controller_nodes[count.index].address # Le endpoint talos
  node                        = local.controller_nodes[count.index].address # Le noeud en question ou on fait un apply
  # Ici je remets 2 patch pour la config DNS et NTP
  config_patches = [
    yamlencode({
      machine = {
        network = {
          hostname    = local.controller_nodes[count.index].name
          nameservers = var.dns_serveurs
        }
        time = {
          servers = var.ntp_serveurs
        }
      }
    }),
  ]
}

# Exactement comme le bloc au dessus mais la en pr√©cisant les workers
resource "talos_machine_configuration_apply" "worker" {
  count                       = var.worker_count
  client_configuration        = talos_machine_secrets.talos.client_configuration
  machine_configuration_input = data.talos_machine_configuration.worker.machine_configuration
  endpoint                    = local.worker_nodes[count.index].address
  node                        = local.worker_nodes[count.index].address
  config_patches = [
    yamlencode({
      machine = {
        network = {
          hostname    = local.worker_nodes[count.index].name
          nameservers = var.dns_serveurs
        }
        time = {
          servers = var.ntp_serveurs
        }
      }
    }),
  ]
}

```

---

# üîß Upgrade du cluster via Terraform

## üÜô Comment je g√®re les upgrades

La gestion des versions est simplissime. Dans mes variables :

```hcl
variable "kubernetes_version" {
  description = "Version de Kubernetes"
  type        = string
  default     = "v1.33.3"
}
```

Je change la version, je `terraform apply`, et Talos d√©tecte la diff√©rence. C‚Äôest tout.

## ‚úÖ Mon retour d‚Äôexp√©rience

Ce que j‚Äôadore :

- Mise √† jour **automatis√©e et progressive**
- **Pas de downtime**
- Possibilit√© de **rollback**
- Terraform garde **l‚Äô√©tat √† jour**

### Points de vigilance

- Lire les **release notes**
- Tester sur un cluster **de dev**
- Toujours **backuper les configs**
- **Observer les logs** pendant l‚Äôupgrade

---

# üìÅ Mon Homelab

Tout est r√©f√©renc√© dans le d√©p√¥t GitHub ci dessous, n'h√©sitez pas y jetez un ≈ìil et une star ‚≠êÔ∏è fait toujours plaisir !

{{< github repo="florianspk/home-lab-talos" >}}

---

# üéâ Bilan apr√®s 1 an avec Talos

## ‚úÖ Les points positifs

- **Simplicit√©** : pas d‚ÄôOS √† configurer
- **S√©curit√©** : syst√®me immuable et minimal
- **API-first** : tout passe par `talosctl`
- **Upgrades** sans stress
- Int√©gration **Terraform + Helm** au top

## ü§î Quelques b√©mols

- **Courbe d‚Äôapprentissage** (pas de shell‚Ä¶)
- **Debug** parfois moins √©vident

## üöÄ Mes conseils

- Testez Talos sur un petit cluster
- G√©rez vos configs avec Terraform
- Int√©grez vos charts Helm d√®s le d√©but

Cette stack **Talos + Terraform + Helm** m‚Äôa vraiment simplifi√© la vie.

**Prochaine √©tape** : creuser **Omni**, l‚Äôoutil de gestion des clusters Talos par SideroLabs üéØ
